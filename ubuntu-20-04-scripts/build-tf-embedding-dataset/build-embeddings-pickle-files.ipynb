{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We get the vocabulary, to know how many \"words\" are all-over in our complete dataset.\n",
    "\n",
    "# Now open every bag-styled-pickle file, for every item ,map \"word\" to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pickle_filenames(pickle_file_dir):\n",
    "    files = os.listdir(pickle_file_dir)\n",
    "    tar_files = list()\n",
    "    for f in files:\n",
    "        if f.endswith(\".pickle\"):\n",
    "            tar_files.append(f)\n",
    "    \n",
    "    return tar_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle_file_content(full_path_pickle_file):\n",
    "    pickle_file = open(full_path_pickle_file,'rb')\n",
    "    pickle_list = pickle.load(pickle_file, encoding='latin1')\n",
    "    pickle_file.close()\n",
    "    \n",
    "    return pickle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(full_path_vocab_file):\n",
    "    pickle_file = open(full_path_vocab_file,'rb')\n",
    "    pickle_list = pickle.load(pickle_file, encoding='latin1')\n",
    "    pickle_file.close()\n",
    "    \n",
    "    return pickle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_to_pickle(full_path_embeddings_file, embeddings_list):\n",
    "    pickle_file = open(full_path_embeddings_file,'wb+')\n",
    "    pickle_list = pickle.dump(embeddings_list, pickle_file)\n",
    "    pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run took: 0:01:50.726590 Hour:Min:Sec\n",
      "size_of_longest_disas: 172578\n",
      "nr_of_disas: 521126\n",
      "embeddings_list: []\n"
     ]
    }
   ],
   "source": [
    "#### main\n",
    "start=datetime.now()\n",
    "\n",
    "bag_styled_file_dir = \"/tmp/savetest\"\n",
    "embedding_styled_file_dir = \"/tmp/embtest\"\n",
    "full_path_vocab_file = \"/tmp/vocab.pickle\"\n",
    "\n",
    "embeddings_list = list()\n",
    "disas_embeddings = list()\n",
    "\n",
    "biggest_nr_of_words_in_disas = 0\n",
    "nr_of_disas = 0\n",
    "\n",
    "### get the vocab\n",
    "vocab = get_vocabulary(full_path_vocab_file)\n",
    "\n",
    "### get list with all files, we want to replace word with embedding-int\n",
    "all_files = get_all_pickle_filenames(bag_styled_file_dir)\n",
    "\n",
    "\n",
    "for file in all_files:\n",
    "    content = get_pickle_file_content(bag_styled_file_dir + '/' + file)\n",
    "    \n",
    "    ### clean for next loop\n",
    "    embeddings_list = []\n",
    "    disas_embeddings = []\n",
    "    \n",
    "    ### loop through all items, and build new list with embedding-ints\n",
    "    for disas,ret_type in content:\n",
    "        nr_of_disas += 1\n",
    "        for disas_item in disas:\n",
    "            vi = vocab[disas_item]\n",
    "            disas_embeddings.append(vi)\n",
    "            #print(f'disas_item:{disas_item} embedding-int:{vi}')\n",
    "              \n",
    "        if biggest_nr_of_words_in_disas < len(disas_embeddings):\n",
    "            biggest_nr_of_words_in_disas = len(disas_embeddings)\n",
    "        embeddings_list.append((disas_embeddings, ret_type))\n",
    "        #break \n",
    "        \n",
    "        disas_embeddings = []     \n",
    "    \n",
    "    ### save every embedding to its own pickle file\n",
    "    save_embeddings_to_pickle(embedding_styled_file_dir + '/' + 'emb-' + file, embeddings_list)\n",
    "    embeddings_list = []\n",
    "    \n",
    "    \n",
    "    #break\n",
    "\n",
    "stop=datetime.now()    \n",
    "print(f'Run took: {stop-start} Hour:Min:Sec')\n",
    "print(f'The biggest disassembly in our dataset got >{biggest_nr_of_words_in_disas}< words in it')\n",
    "print(f'We got >{nr_of_disas}< disassemblies in our dataset')\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_of_longest_disas: 14210793\n"
     ]
    }
   ],
   "source": [
    "print(f'size_of_longest_disas: {biggest_nr_of_words_in_disas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
